# -*- coding: utf-8 -*-
"""Copy of EmailSpamClassifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BPHsfyRFMkCeptm6tASlLX3D3hfwM06M
"""

"""
Dataset: https://www.kaggle.com/venky73/spam-mails-dataset

Use the scikit-learn and nltk packages to build a Naive Bayes classifier that determines whether or not an email is spam.. Then run your classifier on some test data to determine its accuracy.
"""

import string

import nltk
import pandas as pd
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB


filepath = "https://static.junilearning.com/ai_level_2/emails.csv"

# Dataset: https://www.kaggle.com/venky73/spam-mails-dataset
# read in data and remove unnecessary columns
data = pd.read_csv(filepath)
data = data.drop('messageid', axis=1)
data = data.drop('label', axis=1)
data.drop_duplicates(inplace=True)

# preprocessing data
nltk.download('stopwords')


def tokenize(text):
	no_punctuation_str = []
	for c in text:
		if c not in string.punctuation:
			no_punctuation_str.append(c)
	no_punctuation_str = ''.join(no_punctuation_str)
	
	nonstop_words = []
	for word in no_punctuation_str.split():
		if word.lower() not in stopwords.words('english'):
			nonstop_words.append(word)
	return nonstop_words


# build feature vectors(X) and label(y) for each vector
# tokenize emails and get counts of each word per email
X = CountVectorizer(analyzer=tokenize).fit_transform(data['text'])
y = data['label_num']

# split into testing and training data
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# create and train Naive Bayes classifier
model = MultinomialNB()
model.fit(x_train, y_train)

# run classifier on the training data and check accuracy
y_pred = model.predict(x_test)
print("Accuracy: " + str(accuracy_score(y_test, y_pred) * 100) + " %")
